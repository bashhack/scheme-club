* Session 1
** Chapter 1
*** Section 1.0 - Introduction to Listp and Computational Processing
**** The 'computational process' is an abstraction  at the core of the computer - 
      it operates on 'data', governed by pattern(s) of rules we call a 'program'.
**** These processes execute programs, barring 'bugs' or 'errors' that may occur.
**** Mindfulness must be applied to the organization of our programs, as well-structured 
      programs may be visualized, act predictably, etc. while those that eschew
      modular design, or are riddled with errors can lead to catastrophic failure.
**** Invented in late 1950s as a 'formalism for reasoning about the use
      of certain kinds of logical expressions, called "recursion equations", as a model
      for computation', Lisp was first conceived by John McCarthy (MIT, 1960)
      in his paper;
      'Recursive Functions of Symbolic Expressions and Their Computation by Machine'

      NOTE: In this context, 'formalism' refers to a 'theory that holds that statements
            of mathematics and logic can be considered to be statements about the
            consequences of certain string manipulation rules.
**** A Lisp "interpreter" is a machine that executes computational processes 
      as defined in the Lisp language.
**** Lisp is an acronym for 'LISt Processing'
**** Lisp has many dialects (including Scheme), representing a diverse and experimental
      history of language features born of necessity, curiousity, and domain.
**** Lisp is useful for the study of computation processes because it is 'an
      excellent medium for studying important programming constructs and data structures
      and for relating them to the linguistic features that support them.'
**** The most important of these features is that Lisp 'procedures' (i.e., a description
      of a process) can be represented and operated on as data - in other languages, like
      Python and JavaScript, we might say that 'everything is an object' whereas the
      zen-like understanding of Lisp is that 'everything is data'
**** Lisp is, ultimately, a flexible language for us to explore fundamental techniques
*** Section 1.1 - The Elements of Programming
    NOTE:
        When we talk about a 'procedure' we are generally speaking of a 'function' -
        we can generally use the terms interchangeably when speaking about Lisp,
        though there are some nuances that may be articulated depending on context.
**** A programming language is a 'framework for organizing our ideas about processes',
     and accomplishes this by affording us:
     
     * primitive expressions (simplest elements of the language, i.e., numbers, strings, etc)
     * means of combination (allow us to transform simple elements into compound elements)
     * means of abstration (allow us to name and manipulate compound elements)

     * 'We deal with two kinds of elements: procedures and data' - (these are really the same!),
       where data represents the 'thing' we want to operate on/against while the procedure defines
       how we may do so.
**** Programming languages help us not only accurately and comprehensively describe primitive data,
      but give us the tools necessary for combining and 'abstracting procedures and data'
*** Section 1.1.1 - Expressions
**** 'Expressions' are typed or defined within a program, and the 'interpreter' displays the result
      of 'evaluating' that expression
**** One example of a primitive expression is an entity of a type 'number'
**** Numeric expressions can be combined with a primitive procedure (i.e., '+' or '*', etc),
      in order to create a compound expression that 'represents the application of the procedure to
      those numbers'
**** Examples: see `1_building_abstractions_with_procedures.ss`
**** Expressions like these are 'combinations' - the parts of the form are (from left to right) are
     the 'operator' and the 'operands' - the value of the combination is found by 'applying' the 
     function/procedure declared by the operator to the 'arguments' which are the values of our
     operands
**** Lisp uses 'prefix notation' (i.e., placing the operator to the left of its operands) as opposed
     to 'infix notation' used in most programming languages. The advantages of this notational system
     arise when handling arbitary numbers of arguments to a function, and when nesting combinations
*** Section 1.1.2 - Naming and the Environment
**** We cannot get far in any programming language without a means of mapping names to values,
     that is, we need a method for creating variables. To do this in Scheme, we use the `define`
     keyword: 
     #+BEGIN_SRC scheme
       (define pi 3.14)
     #+END_SRC

     #+RESULTS:
     : #<void>

**** Computational objects may have arbitrarily complex structure/form, though we must weigh
      such complexity against added potential for computational or cognitive overhead
**** The 'memory' that stores/tracks name-object pairs is known as the 'global environment'
*** Section 1.1.3 - Evaluating Combinations
**** Evaluating a combination consists of:
      * Evaluating the subexpressions of the combination
      * Applying the procedure/function that is the value of the leftmost subexpression (operator)
        to the arguments that are the values of the other subexpressions (operands)

      NOTE: This process of combinatorial evaluation is akin to the 'beta reduction' found in 
            the lambda calculus (or Haskell), though this is an inexact comparison as the mechanism 
            of 'reduction' differs in some key aspects to Lisp's 'compilation/evaluation'
**** When we notate Lisp program text, we use 'symbolic expression' or 's-expressions/sexprs'.
      Lisp programs are valid sexprs, but not all sexprs are valid Lisp programs.
      Diagramming sexprs leads to clear tree structures (nodes + branches) - as is shown in 
      Figure 1.1:

      ;; An expression, or more completely, a Lisp s-expression
      #+BEGIN_SRC scheme
        (* (+ 2 (* 4 6)) (+ 3 5 7))
      #+END_SRC

      #+RESULTS:
      : 390

      ;; Notated s-expression
      ;; 
      ;;               390_______
      ;;              /  |       \__
      ;;             *   26         15
      ;;               / | \     / | \  \
      ;;             +   2  24  +  3  5  7
      ;;                   / | \
      ;;                  *  4  6
**** Some names, like `define` do not follow the expected rules of evaluation - these
      exceptions are called 'special forms'. These have their own evaluation rules -
      that is, a form like `define` in `(define x 3)` does not apply `define` to the 
      arguments `x` and `3`, its purpose is to associate a value with a name. It is worth
      noting that the form above is also not a combination.
*** Section 1.1.4 - Compound Procedures
**** Compound procedures allow us to express powerful abstractions with ease
**** Compound procedures are simply procedures that are user-defined - this is in contrast
      to primitive procedures owned by the language itself
**** A procedure (function) has the general form: `(define (<name> <formal parameter>) <body>)`
*** Section 1.1.5 - The Substitution Model for Procedure Application
**** In evaluating a combination where the operator is a compount procedure, 
      the interpreter follows a process similar to primitive procedures. See 1.1.3 above.
**** Formally, we describe this process as the 'substitution model' for procedure application.
      It's primary function is to determine the 'meaning' of the procedure application.
**** In the description of evaluation provided in 1.1.3, the interpreter evaluates the 
      operator first, then moves on to evaluate the operands, finally applying the 
      'resulting procedure to the resulting arguments' - this is known as 
      'applicative-order evaluation'. In contrast, there also exists 'normal-order evaluation'.
      In this variant, we 'fully expand and then reduce'. Lisp uses 'applicative-order',
      in part, because it avoids duplication of expression evaluation and because
      the complexity of normal-order evaluation can become quite difficult
      depending on the procedure content. We also know normal-order evaluation as 'lazy' evaluation,
      or 'call by name'. Applicative-order evaluation we commonly refer to as 'eager' evaluation,
      or 'call by value'.
*** Section 1.1.6 - Conditional Expressions and Predicates
**** When we want to test against a series of predicates (i.e., 'case analysis'), 
      we use the special form `cond` where:
     #+BEGIN_SRC scheme
       ;; (cond (<p1> <e1>)
       ;;       (<p2> <e2>)
       ;;       ...
       ;;       (<pn> <en>))
     #+END_SRC

     #+RESULTS:

**** Predicates are evaluated from 'top to bottom', with the interpreter looking
      for a `true` value (then returning the value of the corresponding expression),
      or returning a value of undefined if no true value is found.
**** `cond`/`if` forms, along with primitive predicates (`<`, `=`, and `>`) and logical
      operators (`and`/`or`/`not`) allow us to write compound predicates
*** Section 1.1.7 - Example: Square Roots by Newton's Method
**** Functions in Scheme are much like functions in maths
**** They map inputs to outputs, or - said another way - potential return values to potential parameters
**** Functions in Scheme, unlike in maths, must be effectively computable
**** The 'contrast between [mathematical] function and procedure is a reflection of the general
     distinction between describing properties of things and describing how to do things ...
     or between declarative knowledge and imperative knowledge .... In mathematics we are
     usually concerned with declarative (what is) ... in computer science we are usually
     concerned with imperative (how to) descriptions'
**** Let's take a look at finding square roots to flesh out this distinction:

     Declarative:
     "Use Newton's successive approximations technique: when we guess 'y' for the value
      of the square root of a number 'x', we can average 'x/y' to get a better guess"

     Imperative:
     #+BEGIN_SRC scheme
      (define sqrt-iter
        (lambda (guess x)
          (if (good-enough? guess x)
              guess
              (sqrt-iter (improve guess x) x))))

      (define improve
        (lambda (guess x)
          (average guess (/ x guess))))

      (define average
        (lambda (x y)
          (/ (+ x y) 2)))

      (define good-enough?
        (lambda (guess x)
          (< (abs (- (square guess) x)) 0.001)))

      (define sqrt
        (lambda (x)
          (sqrt-iter 1.0 x)))
     #+END_SRC
     
*** Section 1.1.8 - Procedures as Black-Box Abstractions
**** `sqrt-iter` provides us with our first real introduction to recursion
**** We should seek to understand and break apart problems into smaller subproblems
**** The process by which we separate a problem into its parts is called decomposition
**** Successful decomposition is to split not arbitrarily points, but at those points 
      where it makes logical sense and where doing so results in each individual part 
      performing a discrete task - in some ways, this echoes the Unix philosophy
      of "do one thing and do it well"
**** To decompose our code in these logical separations is to abstract our code
      via 'procedural abstraction'
**** Procedural abstraction allows us to 'suppress implementation detail'
**** One such detail that should never really matter to the user of a procedure 
      is the names given to formal parameters, so long as their names remain consistent
      throughout the body of the procedure
**** When a value is given to a formal parameter, that entity is known as a 
      bound variable - that is, the procedure definition has binds its formal parameters
      to corresponding values. If a variable is not bound, we say that it is 'free'.
**** The expressions 'for which a binding defines a name' is known as 'scope'
**** In the following expression, `guess` and `x` are bound, `<`, `-`, `abs` and `square` are free:
     
     #+BEGIN_SRC scheme
       (define good-enough?
         (lambda (guess x)
           (< (abs (- (square guess) x)) 0.001)))
     #+END_SRC
**** We can leverage 'block structure' in order to internalize our definitions. 
     See: `1_building_abstractions_with_procedures.ss`
**** Similarly, we can combine block structure with lexical scoping to allow `x` to
      be a free variable, each procedure refering to the `x` of our enclosing
      proceure `sqrt`
*** Section 1.2 - Procedures and the Processes They Generate
**** 'The ability to visualize the consequences of the actions under
      consideration is crucial to becoming an expert programmer.'
**** 'A procedure is a pattern for the "local evolution" of a
      computational process.'
**** 'We would like to be able to make statements about the overall,
      or "global", behavior of a process whose local evolution has
      been specified by a procedure.'
**** To do this, we'll examine 'common "shapes" for processes generated
      by simple procedures' as well as their 'computational resources
      of time and space' - in other words, we'll look at scale analysis
      (i.e., Big O notation)
*** Section 1.2.1 - Linear Recursion and Iteration
**** Factorials offer many valid computational approaches
**** One such way is to understand that `n!` is equivalent to `n * (n - 1)!`
     We can translate this into code with ease:

     #+BEGIN_SRC scheme
       (define factorial
         (lambda (n)
           (if (= n 1)
               1
               (* n (factorial (- n 1))))))
     #+END_SRC
**** By using our substitution model (i.e., applicative-order evaluation),
     we can visualize the execution of this procedure

     #+BEGIN_SRC scheme
       (factorial 6)
       (* 6 (factorial 5))
       (* 6 (* 5 (factorial 4)))
       (* 6 (* 5 (* 4 (factorial 3))))
       (* 6 (* 5 (* 4 (* 3 (factorial 2)))))
       (* 6 (* 5 (* 4 (* 3 (* 2 (factorial 1))))))
       (* 6 (* 5 (* 4 (* 3 (* 2 1)))))
       (* 6 (* 5 (* 4 (* 3 2))))
       (* 6 (* 5 (* 4 6)))
       (* 6 (* 5 24))
       (* 6 120)
       720
     #+END_SRC
**** We could also describe computing a factorial `n!` as
     'we first multiply 1 by 2, then multiply the result by 3,
     then by 4, and so on until we reach n'. In other words,
     we keep track of a counter (from 1 to n) and a running product.

     #+BEGIN_SRC scheme
       (define factorial-linear-iter
         (lambda (n)
           (letrec ((iter
                     (lambda (product counter)
                       (if (> counter n)
                           product
                           (iter (* counter product)
                                 (+ counter 1))))))
             (iter 1 1))))
     #+END_SRC
**** Both the linear recursive and linear iterative approach are valid -
     but their fundamental approach differs, as do their 'shapes'
**** In the recursive case, the shape is one of expansion then contraction,
     building a 'chain of deferred operations' contracting when the operations
     are actually executed.

     The length of the 'chain of deferred multiplications, and hence the amount
     of information needed to keep track of it, grows linearly with n (is proportional
     to n), just like the number of steps.' This is called a 'linear recursive process'.
**** In the iterative case, at each step we track all necessary state, resulting
     in the flat shape that does not grow or shrink over the course of its evaluation.
     An 'iterative process is one whose state can be summarized by a fixed number
     of state variables, together with a fixed rule that describes how the state
     variables should be updated as the process moves from state to state and
     an (optional) end test that specifies conditions under which the
     process should terminate.'

     The number of steps required grows linearly with n. Such a process is called
     a 'linear iterative process'.
**** We should be 'careful not to confuse the notion of a recursive process with
     the notion of a recursive procdure' - speaking of a recursive procedure is
     refers to the fact that a procedure references (directly or indirectly) itself.
**** In other languages (Pascal, Ada, and C) - a recursive procedure consumes
     more and more memory as the number of procedure calls increases - even when
     the process 'described is, in principle, iterative.'
**** Using a 'tail-recursive' implementation of a language allows us to
     execute an 'iterative process in constant space, even if the iterative process
     is described by a recursive procedure'
*** Section 1.2.2 - Tree Recursion
**** Another pattern of computation is 'tree recursion',
     where leaf nodes are themselves recursive
**** There is the possibility that as process evolves, nodes
     will repeat recursive work performed by other nodes
**** A textbook example of tree recursion (and its performance
     downsides) is given below:

     #+BEGIN_SRC scheme
       (define fib
         (lambda (n)
           (cond ((= 0) 0)
                 ((= n 1) 1)
                 (else (+ (fib (- n 1))
                          (fib (- n 2)))))))
        (fib 5)
     #+END_SRC
**** In general, 'the number of steps required by a tree-recursive process will
     be proportional to the number of nodes in the tree, while the space required
     will be proportional to the maximum depth of the tree.'
**** An iterative solution to expressing the Fibonacci sequence can be written as:

     #+BEGIN_SRC scheme
       (define fib-iter
         (lambda (a b count)
           (if (= count 0)
               b
               (fib-iter (+ a b) a (- count 1)))))

       (define fib
         (lambda (n)
           (fib-iter 1 0 n)))
     #+END_SRC
*** Section 1.2.3 - Orders of Growth
**** We can talk about computational resources in many ways. One of the 
     most common methods to do so is using 'Big O/Little o/Theta/Omega' notation - 
     a notation that allows us to talk about/reason about 'order of growth'.
**** Orders of growth frame resource usage by a process as inputs become larger
**** Thinking about expressing orders of growth begins by establishing
     some initial values:
     
     - let n be the size of the problem
     - let R(n) be the resources a process requires for a problem of size n
     - R(n) has an order of growth Theta(f(n)), in other words R(n) = Theta(f(n))
       - moreover, we would say that Theta(f(n)) = k1f(n) <= R(n) <= k2f(n)
**** In our linear recursive process, we would say that our process:
     
     - Steps required for the process grows as Theta(n)
     - Space required for the process grows as Theta(n)
**** In our linear iterative process, the space requirements were much different:
     
     - Steps required for the process grows as Theta(n) (linear)
     - Space required for the process grows as Theta(1) (constant)
**** Orders of growth 'provide only a crude description of the behavior of a process'
*** Section 1.2.4 - Exponentiation
**** Exponentiation can be expressed recursively as:
     b^n = b * b^(n - 1)
**** We can translate this easily into Scheme:
     
      #+BEGIN_SRC scheme
        (define (expt b n)
          (if (= n 0)
              1
              (* b (expt b (- n 1)))))
      #+END_SRC
**** This linear recursive solution has step and space requirements of `O(n)`
**** A linear iterative solution can also be expressed as:
    
      #+BEGIN_SRC scheme
        (define expt-iter
          (lambda (b counter product)
            (if (= counter 0)
                product
                (expt-iter b (- counter 1) (* b product)))

        (define expt
          (lambda (b n)
            (expt-iter b n 1)))

        ;; (expt-iter 2 3 1)
        ;; (expt-iter 2 2 2)
        ;; (expt-iter 2 1 4)
        ;; (expt-iter 2 0 8)
      #+END_SRC
**** By leveraging successive squaring we can optimize our procedure:
     
      b^n = (b^(n/2))^2 (if n is even)
      b^n = b * b^(n - 1) (if n is odd)
**** Translating this to Scheme, we can create a fast-exponentiation procedure:

      #+BEGIN_SRC scheme
        (define square
          (lambda (x)
            (* x x)))

        (define even?
          (lambda (n)
            (= (remainder n 2) 0)))

        (define fast-expt
          (lambda (b n)
            (cond ((= n 0) 1)
                  ((even? n) (square (fast-expt b (/ n 2))))
                  (else (* b (fast-expt b (- n 1)))))))

        (fast-expt 2 2)
      #+END_SRC

**** This new fast-exponentiation procedures allows the size of the exponent 
      to double for each new multiplication, essentially reducing to a 
      step and space complexity of O(log n)
*** Section 1.2.5 - Greatest Common Divisors
**** The greatest common divisor (GCD) of two integers a and b is defined
      to be the largest integer that divides both a and b with no remainder.
**** We need to know how to compute GCDs in order to reduce 
      rational numbers to their lowest terms.

      To reduce a rational number to lowest terms, we must divie both
      the numerator and the denominator by their GCD. For example,
      16/28 reduces to 4/7.
**** We can find the GCD by factoring two integers and searching for common factors,
      however, there is a much more efficient algorithm from Euclid. The algorithm
      first appears in Euclid's 'Elements' (Book 7, ca. 300 B.C.). Per Donald Knuth (1973),
      it can be considered the oldest non-trivial algorithm. The ancient Egyptian 
      fast-mult-iter from Exercise 1.18 is older, but Knuth explains that it was
      a set of illustrative examples whereas Euclid's Algorithm is presented as
      a general algorithm.
**** Euclid's Algorithm is based on the observation that:

      If 'r' is the remainder when 'a' is divided by 'b',
      then the common divisors of 'a' and 'b' are
      precisely the same as the common divisors of 'b' and 'r'.
      
**** Succinctly, GCD(a,b) = GCD(b,r)
**** Using this observation, we reduce the problem to computing the GCD of smaller
      and smaller pairs of integers. For example:
      
      GCD(206, 40) = GCD(40, 6)
                   = GCD(6, 4)
                   = GCD(4, 2)
                   = GCD(2, 0)
                   = 2
**** We can express Euclid's Algorithm in code, as:
     
     #+BEGIN_SRC scheme
       (define gcd
         (lambda (a b)
           (if (= b 0)
               a
               (gcd b (remainder a b)))))
       
       (gcd 206 40)
     #+END_SRC
**** This generates an iterative process whose steps grow as the log of the numbers involved
**** Lame's Theorem:
      Gabriel Lame (a French mathematician and engineer known for his 
      work in mathematical physics), proved this theorem in 1845.
      
      "If Euclid's Algorithm requires 'k' steps to compute the GCD of some pair,
       then the smaller number in the pair must be greater than or equal
       to the kth Fibonacci number."

      This theorem allows us to compute an order-of-growth estimate for Euclid's Algorithm.
      Let 'n' be the smaller of two inputs to the procedure. If the process takes
      'k' steps, then we know:

      n >= Fib(k) or phi^k / sqrt(5)
      
      Therefore, the number of steps 'k' grows as the logarithm (to the base of phi) of 'n',
      or \theta(log n).
*** Section 1.2.6 - Example: Testing for Primality
**** We aim to explore two methods for checking the primality of an integer n,
      one with order of growth \theta(sqrt(n)), and a 'probabilistic' algorithm
      with order of growth \theta(log n).
**** 
